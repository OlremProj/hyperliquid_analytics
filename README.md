# ğŸš€ Hyperliquid Analytics Agent

Agent dâ€™analyse technique en cours de construction autour des donnÃ©es Hyperliquid, avec une trajectoire orientÃ©e vers lâ€™agrÃ©gation multichaÃ®ne on-chain et des calculs off-chain avancÃ©s.

## Ã‰tat actuel

- **Client Hyperliquid (async)** : appels `/info` (`candleSnapshot`, `metaAndAssetCtxs`, `userFills`) gÃ©rÃ©s, avec journalisation et validations Pydantic (`PerpMeta`, `PerpAssetContext`, `MetaAndAssetCtxsResponse`, `MarketData`â€¦).
- **Repository DuckDB** : schÃ©ma persistant pour `perp_universe`, `margin_tables`, `perp_asset_ctxs`, transactions explicites, accÃ¨s `fetch_latest` & `fetch_history`, timestamp UTC automatique.
- **Services** :
  - `AnalyticsService` orchestre le client Hyperliquid et le repository (ingestion async via `to_thread`, lectures latest/history).
  - `IndicatorService` calcule en 100â€¯% SQL (DuckDB) les indicateurs SMA, EMA, RSI, MACD, Bollinger, ATR, Stochastic et VWAP.
  - `AnalysisPipeline` (Nouveau) : Structure pour exÃ©cuter des stratÃ©gies (`BaseStrategy`) et gÃ©nÃ©rer des `Signal`.
- **CLI Click** : commandes `collect snapshot`, `collect candles`, `show latest`, `show history`, `show indicator` avec option globale `--db-path`, sorties JSON prÃªtes pour piping.
- **Tests unitaires** : couverture des modÃ¨les, client, services (ingestion & indicateurs), repository, CLI ; suite Pytest paramÃ©trÃ©e.

## Installation

```bash
python3 -m venv venv
source venv/bin/activate
pip install -e ".[dev]"
```

## Configuration

```bash
cp .env.example .env
# Ã‰diter .env avec vos clÃ©s/URL Hyperliquid
```

Variables principales :

- `HYPERLIQUID_ANALYTICS_BASE_URL=https://api.hyperliquid.xyz`
- `HYPERLIQUID_ANALYTICS_SYMBOLS_RAW=BTC,ETH` (etc.)
- `HYPERLIQUID_ANALYTICS_API_KEY` si nÃ©cessaire pour des endpoints privÃ©s.

## Commandes utiles

```bash
# Collecter un snapshot complet et lâ€™enregistrer en DuckDB
python -m hyperliquid_analytics.cli collect snapshot

# Collecter des bougies OHLCV (ex : 200x 1h sur BTC)
python -m hyperliquid_analytics.cli collect candles -s BTC -t 1h -l 200
# La commande vÃ©rifie d'abord la derniÃ¨re bougie stockÃ©e et ne rapatrie
# qu'en cas de gap > 1 intervalle (sauf si --limit force un backfill).

# Dernier snapshot pour un symbole
python -m hyperliquid_analytics.cli show latest -s BTC

# Historique rÃ©cent (20 entrÃ©es par dÃ©faut)
python -m hyperliquid_analytics.cli show history -s BTC --limit 5

# Calculer un indicateur (ex : SMA 20 pÃ©riodes en 1h)
python -m hyperliquid_analytics.cli show indicator sma -s BTC -t 1h --window 20

# Indicateurs disponibles (nov. 2025) : sma, ema, rsi, macd, bollinger, atr, stochastic, vwap

# Ingestion temps rÃ©el (WebSocket bougies + backfill auto)
python -m hyperliquid_analytics.cli scheduler ws -t 5m
# Maintient DuckDB Ã  jour Ã  partir du flux Hyperliquid et rattrape les trous via REST

# Scheduler basique (collecte pÃ©riodique)
python -m hyperliquid_analytics.cli scheduler run -t 1h -t 4h --interval 300 --iterations 0 --snapshot
# `--interval` relance la collecte toutes les 5 minutes, `--iterations 0` = boucle infinie (Ctrl+C pour arrÃªter)
# utilise les symboles dÃ©finis dans .env et journalise chaque collecte

# SpÃ©cifier un autre fichier DuckDB
python -m hyperliquid_analytics.cli --db-path data/dev.duckdb collect snapshot
```

## Tests

```bash
./venv/bin/python -m pytest
```

Astuce : exÃ©cuter `pip install -e .[dev]` avant les tests pour sâ€™assurer que le package est importable avec le layout `src/`.

## Roadmap vers un systÃ¨me dâ€™analytics complet

- **ğŸ£ Phase 1 â€” Hyperliquid seulement (en cours)**
  - [x] Client async & modÃ¨les Pydantic
  - [x] Service + CLI de collecte/lecture
  - [x] Tests unitaires Repository / CLI / Scheduler
  - [x] Indicateurs de base (SMA/EMA, RSI, MACD, Bollinger) via DuckDB
  - [x] Extensions indicateurs : ATR, Stochastic, VWAP (calculs 100 % SQL sur `candles`)
  - [x] Scheduler dâ€™ingestion pÃ©riodique (CLI `scheduler run`)
  - [x] Architecture Pipeline (`AnalysisPipeline`, `BaseStrategy`, `Signal`)
  - [ ] Alertes locales + jobs dÃ©diÃ©s (analysis pipeline)

- **ğŸŒ Phase 2 â€” Analytics temps rÃ©el & API interne**
  - [ ] **Architecture dÃ©couplÃ©e** : `scheduler ws` (collecte seule) vs `strategy run` (analyse parallÃ¨le).
  - [ ] WebSocket trades / L2 book + stockage incrÃ©mental
  - [ ] API FastAPI exposant snapshots & indicateurs
  - [ ] Tableau de bord (Streamlit / front custom)
  - [ ] Alerting (funding extrÃªme, variations OI, divergence volume/prix)

- **ğŸ”— Phase 3 â€” Extension multichaÃ®ne & on-chain**
  - [ ] Ingestion donnÃ©es on-chain (DEX, bridges, mÃ©triques DeFi)
  - [ ] CorrÃ©lations funding / flux on-chain
  - [ ] Normalisation multi-sources, enrichissement du repository
  - [ ] Archivage Parquet + politiques de rÃ©tention

- **ğŸš€ Phase 4 â€” Industrialisation**
  - [ ] Migration possible vers TimescaleDB / ClickHouse
  - [ ] Pipelines distribuÃ©s, observabilitÃ© & monitoring
  - [ ] Modules analytiques avancÃ©s (backtesting, signaux ML)

## Prochaines Ã©tapes

1.  **`scheduler ws`** : Optimiser pour l'ingestion pure (sans calculs bloquants).
2.  **`strategy run`** : Nouvelle commande dÃ©diÃ©e qui surveille la DB et exÃ©cute les stratÃ©gies en asynchrone.
3.  **Tests** : Ajouter des mocks pour valider le flux complet WS -> DB -> Strategy.

---

ğŸ‘‰ Contributions / feedback bienvenus : tests, intÃ©grations de nouvelles sources, idÃ©es dâ€™indicateurs. Ouvre une issue ou une PR pour en discuter ! ğŸ’¬
